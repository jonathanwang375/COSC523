# Import Libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier

# Load Data
train_data = pd.read_csv("/kaggle/input/titanic/train.csv")
test_data = pd.read_csv("/kaggle/input/titanic/test.csv")

# Data Preprocessing and Feature Engineering
def preprocess_data(data):
    # Impute missing values for Age and Fare with median, Embarked with mode
    data['Age'] = data['Age'].fillna(data['Age'].median())
    data['Fare'] = data['Fare'].fillna(data['Fare'].median())
    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])
    
    # Extract Deck from Cabin and count the number of cabins per passenger
    data['Deck'] = data['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'M')
    data['CabinCount'] = data['Cabin'].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)
    
    # Extract Title from Name
    data['Title'] = data['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())
    
    # Family Size and IsAlone
    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1
    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)
    
    # Ticket Frequency (number of passengers with the same ticket)
    ticket_counts = data['Ticket'].value_counts()
    data['TicketFreq'] = data['Ticket'].map(ticket_counts)
    
    # Bin Age and Fare
    data['AgeBin'] = pd.qcut(data['Age'].fillna(data['Age'].median()), 4, labels=False)
    data['FareBin'] = pd.qcut(data['Fare'].fillna(data['Fare'].median()), 4, labels=False)
    
    # Encode categorical features
    label_cols = ['Sex', 'Embarked', 'Deck', 'Title']
    for col in label_cols:
        data[col] = LabelEncoder().fit_transform(data[col].astype(str))
    
    # Drop irrelevant columns
    return data.drop(['Name', 'Cabin', 'Ticket', 'Age', 'Fare', 'PassengerId'], axis=1, errors='ignore')

# Apply preprocessing to training and test data
train_data = preprocess_data(train_data)
test_data = preprocess_data(test_data)
X, y = train_data.drop('Survived', axis=1), train_data['Survived']

# Standardize Numerical Features
scaler = StandardScaler()
X[['FamilySize', 'TicketFreq', 'CabinCount']] = scaler.fit_transform(X[['FamilySize', 'TicketFreq', 'CabinCount']])
test_data[['FamilySize', 'TicketFreq', 'CabinCount']] = scaler.transform(test_data[['FamilySize', 'TicketFreq', 'CabinCount']])

# Model Definitions with increased max_iter for LogisticRegression
logistic_model = LogisticRegression(max_iter=2000, random_state=1)
rf_model = RandomForestClassifier(random_state=1)
gb_model = GradientBoostingClassifier(random_state=1)
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=1)

# Hyperparameter Tuning Function
def tune_model(model, param_grid):
    search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
    search.fit(X, y)
    return search.best_estimator_

# Parameter grids for tuning
rf_params = {'n_estimators': [100, 200], 'max_depth': [5, 10, None], 'min_samples_split': [2, 5]}
gb_params = {'n_estimators': [150, 200], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 4, 5]}
xgb_params = {'n_estimators': [100, 150], 'learning_rate': [0.05, 0.1], 'max_depth': [3, 4, 5]}

# Tune and Evaluate Models Individually
# Logistic Regression
logistic_cv_score = cross_val_score(logistic_model, X, y, cv=5, scoring='accuracy').mean()

# Gradient Boosting
best_gb_model = tune_model(gb_model, gb_params)
gb_cv_score = cross_val_score(best_gb_model, X, y, cv=5, scoring='accuracy').mean()

# XGBoost
best_xgb_model = tune_model(xgb_model, xgb_params)
xgb_cv_score = cross_val_score(best_xgb_model, X, y, cv=5, scoring='accuracy').mean()

# Stacking Ensemble Model with Optimized Models
stacking_model = StackingClassifier(
    estimators=[('logistic', logistic_model), ('gb', best_gb_model), ('xgb', best_xgb_model)],
    final_estimator=RandomForestClassifier(n_estimators=100, random_state=1),
    cv=5
)
stacking_cv_score = cross_val_score(stacking_model, X, y, cv=5, scoring='accuracy').mean()

# Display CV Accuracies
print(f"Logistic Regression CV Accuracy: {logistic_cv_score:.4f}")
print(f"Optimized Gradient Boosting CV Accuracy: {gb_cv_score:.4f}")
print(f"Optimized XGBoost CV Accuracy: {xgb_cv_score:.4f}")
print(f"Ensemble Model CV Accuracy: {stacking_cv_score:.4f}")

# Train Final Stacking Model on Full Data and Make Predictions for Submission
stacking_model.fit(X, y)
final_predictions = stacking_model.predict(test_data)

# Save Predictions to CSV
submission = pd.DataFrame({'PassengerId': pd.read_csv("/kaggle/input/titanic/test.csv")['PassengerId'], 'Survived': final_predictions})
submission.to_csv('stacking_submission.csv', index=False)
print("Submission saved as stacking_submission.csv")
