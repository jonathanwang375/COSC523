# Import Libraries
import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier
from sklearn.model_selection import RandomizedSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.impute import SimpleImputer
from xgboost import XGBClassifier

# Load Data
train_data = pd.read_csv("/kaggle/input/titanic/train.csv")
test_data = pd.read_csv("/kaggle/input/titanic/test.csv")

# Data Preprocessing and Feature Engineering
def preprocess_data(data):
    # Impute missing values for Age and Fare with median, Embarked with mode
    data['Age'] = data['Age'].fillna(data['Age'].median())
    data['Fare'] = data['Fare'].fillna(data['Fare'].median())
    data['Embarked'] = data['Embarked'].fillna(data['Embarked'].mode()[0])
    
    # Extract Deck from Cabin and count the number of cabins per passenger
    data['Deck'] = data['Cabin'].apply(lambda x: x[0] if pd.notnull(x) else 'M')
    data['CabinCount'] = data['Cabin'].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)
    
    # Extract Title from Name
    data['Title'] = data['Name'].apply(lambda name: name.split(',')[1].split('.')[0].strip())
    
    # Family Size and IsAlone
    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1
    data['IsAlone'] = (data['FamilySize'] == 1).astype(int)
    
    # Ticket Frequency (number of passengers with the same ticket)
    ticket_counts = data['Ticket'].value_counts()
    data['TicketFreq'] = data['Ticket'].map(ticket_counts)
    
    # Feature Interactions and Binning
    data['Age*Pclass'] = data['Age'] * data['Pclass']
    data['FareBin'] = pd.qcut(data['Fare'].fillna(data['Fare'].median()), 4, labels=False)
    data['AgeBin'] = pd.qcut(data['Age'].fillna(data['Age'].median()), 4, labels=False)
    
    # Encode categorical features
    label_cols = ['Sex', 'Embarked', 'Deck', 'Title']
    for col in label_cols:
        data[col] = LabelEncoder().fit_transform(data[col].astype(str))
    
    return data.drop(['Name', 'Cabin', 'Ticket', 'Age', 'Fare', 'PassengerId'], axis=1, errors='ignore')

# Apply preprocessing to training and test data
train_data = preprocess_data(train_data)
test_data = preprocess_data(test_data)
X, y = train_data.drop('Survived', axis=1), train_data['Survived']

# Standardize Numerical Features
scaler = StandardScaler()
X[['FamilySize', 'TicketFreq', 'CabinCount', 'Age*Pclass']] = scaler.fit_transform(X[['FamilySize', 'TicketFreq', 'CabinCount', 'Age*Pclass']])
test_data[['FamilySize', 'TicketFreq', 'CabinCount', 'Age*Pclass']] = scaler.transform(test_data[['FamilySize', 'TicketFreq', 'CabinCount', 'Age*Pclass']])

# Model Definitions with increased max_iter for LogisticRegression
logistic_model = LogisticRegression(max_iter=3000, random_state=1)
rf_model = RandomForestClassifier(random_state=1)
gb_model = GradientBoostingClassifier(random_state=1)
xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=1)

# Hyperparameter Tuning with RandomizedSearchCV for faster search over a wider parameter space
def tune_model(model, param_dist, n_iter=15):
    search = RandomizedSearchCV(model, param_dist, n_iter=n_iter, cv=5, scoring='accuracy', n_jobs=-1, random_state=1)
    search.fit(X, y)
    return search.best_estimator_

# Parameter distributions for tuning
rf_params = {'n_estimators': [100, 200, 300], 'max_depth': [5, 10, 15, None], 'min_samples_split': [2, 5, 10]}
gb_params = {'n_estimators': [100, 150, 200], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 4, 5]}
xgb_params = {'n_estimators': [100, 150, 200], 'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3, 4, 5]}

# Tune Random Forest, Gradient Boosting, and XGBoost models
best_rf_model = tune_model(rf_model, rf_params)
best_gb_model = tune_model(gb_model, gb_params)
best_xgb_model = tune_model(xgb_model, xgb_params)

# Ensemble Model with Voting and Stacking
voting_model = VotingClassifier(
    estimators=[('logistic', logistic_model), ('rf', best_rf_model), ('gb', best_gb_model), ('xgb', best_xgb_model)],
    voting='soft'
)

# Stacking Model with Voting as Meta-Estimator
stacking_model = StackingClassifier(
    estimators=[('voting', voting_model)],
    final_estimator=RandomForestClassifier(n_estimators=100, random_state=1),
    cv=5
)

# Evaluate Individual and Ensemble Model CV Accuracies
logistic_cv_score = cross_val_score(logistic_model, X, y, cv=5, scoring='accuracy').mean()
gb_cv_score = cross_val_score(best_gb_model, X, y, cv=5, scoring='accuracy').mean()
xgb_cv_score = cross_val_score(best_xgb_model, X, y, cv=5, scoring='accuracy').mean()
rf_cv_score = cross_val_score(best_rf_model, X, y, cv=5, scoring='accuracy').mean()
voting_cv_score = cross_val_score(voting_model, X, y, cv=5, scoring='accuracy').mean()
stacking_cv_score = cross_val_score(stacking_model, X, y, cv=5, scoring='accuracy').mean()

# Display CV Accuracies
print(f"Logistic Regression CV Accuracy: {logistic_cv_score:.4f}")
print(f"Optimized Random Forest CV Accuracy: {rf_cv_score:.4f}")
print(f"Optimized Gradient Boosting CV Accuracy: {gb_cv_score:.4f}")
print(f"Optimized XGBoost CV Accuracy: {xgb_cv_score:.4f}")
print(f"Voting Ensemble CV Accuracy: {voting_cv_score:.4f}")
print(f"Stacking Ensemble CV Accuracy: {stacking_cv_score:.4f}")

# Train Final Stacking Model on Full Data and Make Predictions for Submission
stacking_model.fit(X, y)
final_predictions = stacking_model.predict(test_data)

# Save Predictions to CSV
submission = pd.DataFrame({'PassengerId': pd.read_csv("/kaggle/input/titanic/test.csv")['PassengerId'], 'Survived': final_predictions})
submission.to_csv('stacking_submission.csv', index=False)
print("Submission saved as stacking_submission.csv")
